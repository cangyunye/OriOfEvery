{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息的定量描述\n",
    "直观理解： \n",
    "若消息发生的概率很大，受信者事先已经有所估计，则该消息的信息量就很小。 \n",
    "若消息发生的概率很小，受信者感觉到很突然，该消息所含有的信息量就很大。 \n",
    "所以信息量和概率联系在了一起，信息量可以表示为概率的函数。那么怎样的函数可以用来描述信息量呢？函数f(p)f(p)应该满足以下条件： \n",
    "1. f(p)应该是概率p的严格单调递减函数， \n",
    "2. 当p=1时，f(p)=0 \n",
    "3. 当p=0时，f(p)=∞\n",
    "4. 两个独立事件的联合信息量应该等于它们信息量之和。 \n",
    "以下是f(p)=−log(p)f(p)=−log(p)的图像，满足以上的所有的要求。\n",
    "\n",
    "# ID3算法熵计算公式\n",
    "\n",
    "1. 选择分类\\\\(x_i\\\\)的概率\n",
    "\n",
    "2. 信息期望值\n",
    "\n",
    "自信息和熵的定义\n",
    "若一个消息xx出现的概率为p，那么这个消息所含有的信息量为 \n",
    "$$I=−log(p)$$\n",
    "\n",
    "上式称为消息x的自信息，自信息有两种含义： \n",
    "1. 当该消息发生之前，表示发生该消息的不确定性， \n",
    "2. 当该消息发生之后，表示消息所含有的信息量。 \n",
    "信源含有的信息量是信源发出的所有可能消息的平均不确定性，香农把信源所含有的信息量称为熵，是指每个符号所含有的信息量（自信息）的统计平均。若X是一个离散随机变量，概率分布为\\\\(p(x)=P(X=x)\\\\),\\\\(x∈X\\\\)，那么XX的熵为 \n",
    "$$H(X)=\\sum_{i}^{N}p(xi)I(xi)=−\\sum_{i}^{N}p(xi)logp(xi)$$\n",
    "\n",
    "\n",
    "$$H(X)=-\\sum_{i=1}^{n}p(x_i)log_2p(x_i)$$\n",
    "\n",
    "\n",
    "# 设计决策树\n",
    "| 依水 | 鳍否 | 鱼否 |\n",
    "| ---- | ---- | ---- |\n",
    "| yes  | yes  | yes  |\n",
    "| yes  | yes  | yes  |\n",
    "| yes  | no  | no   |\n",
    "| no  | yes  | no   |\n",
    "| no  | no  | no   |\n",
    "\n",
    "1. 从目的导向，先看最终决策项的信息熵 \\\\(H(X)\\\\)  --求先验熵，即求**鱼否**的信息熵。\n",
    "$$H(鱼)=-\\frac{2}{5}log_2\\frac{2}{5}$$\n",
    "$$H(非鱼)=-\\frac{3}{5}log_2\\frac{3}{5}$$\n",
    "$$H(鱼判别)=H(鱼)+H(非鱼)$$\n",
    "\n",
    "2. 已处于某条件类下的各项，--计算后验熵，即求条件中每项成立的各个熵\n",
    "$$H(鱼否|鳍)=-\\frac{2}{3}log_2\\frac{2}{3}-\\frac{1}{3}log_2\\frac{1}{3}$$\n",
    "$$H(鱼否|无鳍)=-\\frac{0}{2}log_2\\frac{0}{2}-\\frac{2}{2}log_2\\frac{2}{2}$$\n",
    "$$H(鱼否|依水)=-\\frac{2}{3}log_2\\frac{2}{3}-\\frac{1}{3}log_2\\frac{1}{3}$$\n",
    "$$H(鱼否|非水)=-\\frac{0}{2}log_2\\frac{0}{2}-\\frac{2}{2}log_2\\frac{2}{2}$$\n",
    "\n",
    "3. 对给定\\\\(Y\\\\)成立的条件下，\\\\(X\\\\)集合求和后验熵即条件熵\\\\(H(X)\\\\)，即求 \\\\(H(鱼否|决策项)\\\\)，即条件熵=\\\\(p_i\\\\)*先验熵\n",
    "$$H(鱼否|鳍否)=H(鱼否|鳍)+H(鱼否|无鳍)$$\n",
    "$$H(鱼否|依水否)=H(鱼否|依水)+H(鱼否|非水)$$\n",
    "\n",
    "4. 信息增益，对**每组判别项与是否结果**的关系\n",
    "$$I(鱼否：鳍否)=H(鱼否)-H(鱼否|鳍否)$$\n",
    "$$I(鱼否：依水否)=H(鱼否)-H(鱼否|依水否)$$\n",
    "\n",
    "先计算先验熵，再计算后验熵，最后计算条件熵\n",
    "\n",
    "\n",
    "按不确定性最小，信息量最大排在树根\n",
    "即，信息增益\\\\(I\\\\)最大的作为树根\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def calcShannonEnt(dataSet): #这个数据集计算香农熵格式要求最后一列数据为判别标签作为H(Label)\n",
    "  numEntries = len(dataSet) #数据集长度\n",
    "  labelCounts = {}\n",
    "  for featVec in dataSet: #对数据集内所有判别式初始化为0\n",
    "    currentLabel = featVec[-1] #区每组数据尾部作为最终判别标签\n",
    "    if currentLabel not in labelCounts.keys():\n",
    "      labelCounts[currentLabel] = 0\n",
    "    labelCounts[currentLabel] += 1\n",
    "  shannonEnt = 0.0 #香农熵\n",
    "  for key in labelCounts:#每个类标签出现概率之和，先验熵\n",
    "    prob = float(labelCounts[key])/numEntries  #计算每个类标签的出现概率\n",
    "    shannonEnt -= prob * log(prob,2) #后验熵 += 后验熵\n",
    "  return shannonEnt #先验熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "  dataSet = [\n",
    "    [1,1,'yes'],\n",
    "    [1,1,'yes'],\n",
    "    [1,0,'no'],\n",
    "    [0,1,'no'],\n",
    "    [0,1,'no']\n",
    "  ]\n",
    "  labels = ['no surfacing','flippers']\n",
    "  return dataSet,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按照给定特征划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet,index,value):#其实这个东西就是用作统计概率用的分数项\n",
    "  retDataSet = []\n",
    "  numFeatures = len(dataSet[0]) - 1\n",
    "  for featVec in dataSet:\n",
    "    if featVec[index] == value:\n",
    "      retDataSet.append([featVec[index],featVec[numFeatures]]) #对应项，对应结果\n",
    "  return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "0.9709505944546686\n",
      "[[1, 'yes'], [1, 'yes'], [1, 'no']] \n",
      " [[0, 'no'], [0, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [1, 'no'], [1, 'no']] \n",
      " [[0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "myDat,labels = createDataSet()\n",
    "print(myDat)\n",
    "sn = calcShannonEnt(myDat)\n",
    "print(sn)\n",
    "x1 = splitDataSet(myDat,0,1)\n",
    "x2 = splitDataSet(myDat,0,0)\n",
    "x3 = splitDataSet(myDat,1,1)\n",
    "x4 = splitDataSet(myDat,1,0)\n",
    "print(x1,'\\n',x2)\n",
    "print(x3,'\\n',x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选择最好的数据集划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet): #要求最后一列为数据为判别标签作为H(Label)\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet) #即得到H(结果)\n",
    "    bestInfoGain, bestFeature = 0.0,-1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet] #判别项 列表值\n",
    "        uniqueVals = set(featList) #取判别项唯一值\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals :#求信息增益\n",
    "            subDataSet = splitDataSet(dataSet,i,value) #对每一列判别项，取每一组值分类\n",
    "            prob = len(subDataSet)/float(len(dataSet)) #求每一组值发生概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet) #条件熵 +=条件熵\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        print('infoGain=',infoGain,'bestFeature=',i,baseEntropy,newEntropy)\n",
    "        if (infoGain > bestInfoGain): #求信息增益最大项\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityCnt(classList): #返回类中最多项的次数\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "            classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)\n",
    "    return myTree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree,featLabels,testVec):\n",
    "    firstStr = inputTree.keys()[0]\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    key = testVec[featIndex]\n",
    "    valueofFeat = secondDict[key]\n",
    "    print('+++',firstStr,'xxx',secondDict,'---',key,'>>>',valueofFeat)\n",
    "    if isinstance(valueofFeat,dict):\n",
    "        classLabel = classify(valueofFeat,featLabels,testVec)\n",
    "    else:\n",
    "        classLabel = valueofFeat\n",
    "    return  classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infoGain= 0.4199730940219749 bestFeature= 0 0.9709505944546686 0.5509775004326937\n",
      "infoGain= 0.17095059445466854 bestFeature= 1 0.9709505944546686 0.8\n",
      "infoGain= 0.0 bestFeature= 0 0.9182958340544896 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "myTree = createTree(myDat,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
