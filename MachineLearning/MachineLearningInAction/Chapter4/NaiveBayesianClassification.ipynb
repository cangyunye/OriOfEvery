{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯\n",
    "优点：数据较少的情况下仍然有效\n",
    "\n",
    "缺点：对于输入数据的准备方式较为敏感\n",
    "## 一般过程：\n",
    "1. 收集数据\n",
    "2. 准备数据：数值型或布尔型\n",
    "3. 分析数据\n",
    "4. 训练算法：计算不同的独立特征的条件概率\n",
    "5. 计算错误率\n",
    "\n",
    "## 原理\n",
    "计算不同的数据独立特征的条件概率\n",
    "$$p(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "区分正类和负类的概率即两种结果的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def loadDataSet(): #词表转换成向量\n",
    "    postingList = [['my','dog','has','flea','problems','help','please'],\n",
    "                  ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','garbage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1表示侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):#取出数据集所有单词标签作为列表\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #“|”求集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):#返回输入文档中单词出现次数统计矩阵\n",
    "    returnVec = [0]*len(vocabList) #数据等长0向量列表作为初始化所有单词次数\n",
    "    for word in inputSet: #遍历inputSet，统计vocabList词汇表中出现的次数\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    #trainCategory：统计的是否侮辱性文本，如[0,1,0,1]，1表示侮辱性文本\n",
    "    numTrainDocs = len(trainMatrix) #训练文本数\n",
    "    numWords = len(trainMatrix[0]) #每个文本中的单词数\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) #侮辱性文本/总文本 ，即侮辱性文本概率\n",
    "    p0Num = ones(numWords) #正常单词统计初始化\n",
    "    p1Num = ones(numWords) #侮辱性单词统计初始化\n",
    "    p0Denom = 2.0 \n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:#侮辱性文本\n",
    "            p1Num += trainMatrix[i] #单词向量统计\n",
    "            p1Denom += sum(trainMatrix[i]) #总单词量统计作为分母\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)#侮辱性文档中，每个侮辱性单词出现概率，用矩阵表示\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    #p0Vec 正常文档\n",
    "    #p1Vec 侮辱性文档\n",
    "    #pClass1 侮辱性文档概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1) #加log部分的前提是p1Vec就是log那么有log(a*b)=loga + logb\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else : \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:#训练统计矩阵，每组文本中单词的出现次数统计\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用朴素贝叶斯对电子邮件进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据\n",
    "def split_mail(mail_file):\n",
    "    import re\n",
    "    words_list = []\n",
    "    pattern = re.compile('\\W+')\n",
    "    try:\n",
    "        with open(mail_file,'r') as fr:\n",
    "            words=re.split(pattern,fr.read())\n",
    "            words_list=[word.lower() for word in words if len(word)>2]\n",
    "    except UnicodeDecodeError:\n",
    "        with open(mail_file,'r', encoding='Windows 1252') as fr:\n",
    "            words=re.split(pattern,fr.read())\n",
    "            words_list=[word.lower() for word in words if len(word)>2]\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the errorCount is : 0\n",
      "the testSet length is : 10\n",
      "the error rate is : 0.0\n"
     ]
    }
   ],
   "source": [
    "def spamTest():\n",
    "    import random\n",
    "    classList = [] #分类标签\n",
    "    docList = [] #文档列表\n",
    "    fullText = []\n",
    "    for fn in range(1,26): #打开目录下所有文件获取词汇\n",
    "        wordList = split_mail('email/ham/%d.txt' % (fn))\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = split_mail('email/spam/%d.txt' % (fn))\n",
    "        docList.append(wordList)\n",
    "#         fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # 词汇表 \n",
    "    vocabList = createVocabList(docList)\n",
    "    testSet = [int(num) for num in random.sample(range(50), 10)]#生成50以内的随机10个数\n",
    "    trainingSet = list(set(range(50)) - set(testSet)) #将训练集剔除测试集\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the errorCount is :',errorCount)\n",
    "    print('the testSet length is :',len(testSet))\n",
    "    print('the error rate is :',float(errorCount)/len(testSet))\n",
    "    \n",
    "spamTest()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'S'), ('1', 'L'), ('1', 'M'), ('1', 'M'), ('1', 'S'), ('2', 'L'), ('2', 'S'), ('2', 'S'), ('2', 'L'), ('2', 'L'), ('2', 'M'), ('3', 'M'), ('3', 'L'), ('3', 'S'), ('3', 'M'), ('3', 'M')]\n",
      "['-1', '1', '1', '-1', '-1', '1', '1', '-1', '1', '-1', '1', '1', '1', '1', '-1', '1']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-33696478ac1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestEntry\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'classified as:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassifyNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthisDoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp0V\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp1V\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpAb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtesting2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-33696478ac1d>\u001b[0m in \u001b[0;36mtesting2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpostinDoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistOPosts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtrainMat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetOfWords2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyVocabList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpostinDoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mp0V\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp1V\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpAb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainNB0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtestEntry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mthisDoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetOfWords2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyVocabList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestEntry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7f0d2afb108b>\u001b[0m in \u001b[0;36mtrainNB0\u001b[1;34m(trainMatrix, trainCategory)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnumTrainDocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMatrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#训练文本数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnumWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#每个文本中的单词数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpAbusive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainCategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumTrainDocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#侮辱性文本/总文本 ，即侮辱性文本概率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mp0Num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumWords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#正常单词统计初始化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mp1Num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumWords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#侮辱性单词统计初始化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1929\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 1930\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   1931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "def data_test():\n",
    "    x1 = ['1','1','1','1','1','2','2','2','2','2','2','3','3','3','3','3']\n",
    "    x2 = ['S','L','M','M','S','L','S','S','L','L','M','M','L','S','M','M']\n",
    "    y = ['-1','1','1','-1','-1','1','1','-1','1','-1','1','1','1','1','-1','1']\n",
    "    x3 = [(x1[z],x2[z]) for z in range(len(x1))]\n",
    "    return x3,y\n",
    "\n",
    "def testing2():\n",
    "    listOPosts,listClasses = data_test()\n",
    "    print(listOPosts)\n",
    "    print(listClasses)\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = [('2','S')] \n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "#应该要改下classifyNB公式，多计算一组概率\n",
    "testing2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = array(['1','1','1','1','1','2','2','2','2','2','2','3','3','3','3','3'])\n",
    "x2 = array(['S','L','M','M','S','L','S','S','L','L','M','M','L','S','M','M'])\n",
    "y = array(['-1','1','1','-1','-1','1','1','-1','1','-1','1','1','1','1','-1','1'])\n",
    "x3=vstack((x1,x2))\n",
    "x3[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ['1','1','1','1','1','2','2','2','2','2','2','3','3','3','3','3']\n",
    "x2 = ['S','L','M','M','S','L','S','S','L','L','M','M','L','S','M','M']\n",
    "y = ['-1','1','1','-1','-1','1','1','-1','1','-1','1','1','1','1','-1','1']\n",
    "x3 = [(x1[z],x2[z]) for z in range(len(x1))]\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
